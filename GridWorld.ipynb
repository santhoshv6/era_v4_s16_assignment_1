{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97494c0c",
   "metadata": {},
   "source": [
    "# GridWorld Value Function (4×4)\n",
    "\n",
    "This notebook computes the state-value function \\(V(s)\\) for a 4×4 GridWorld under a uniform random policy (each action chosen with probability 0.25).  \n",
    "Rewards are -1 per action step, the terminal state (bottom-right) has value 0, \\(\\gamma = 1\\), and iteration stops when the maximum change in \\(V\\) falls below \\(1e{-4}\\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da952b30",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Imports and configuration values used throughout:\n",
    "- Grid size \\(N\\), terminal state index\n",
    "- Discount factor \\(\\gamma\\)\n",
    "- Convergence threshold \\(\\theta\\)\n",
    "- Action set (Up/Down/Left/Right) with equal probability\n",
    "- NumPy print formatting for the final matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1a60705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# Configuration\n",
    "# ----------------------------\n",
    "N = 4                      # grid size N x N\n",
    "GAMMA = 1.0                # no discounting\n",
    "THETA = 1e-4               # convergence threshold\n",
    "TERMINAL_STATE = N * N - 1 # bottom-right\n",
    "\n",
    "# Actions: up, down, left, right (row_delta, col_delta)\n",
    "ACTIONS = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "ACTION_PROB = 1.0 / len(ACTIONS)  # equal probability for each move\n",
    "\n",
    "# Print like the expected output\n",
    "np.set_printoptions(precision=8, suppress=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474d1cbf",
   "metadata": {},
   "source": [
    "## Transition dynamics\n",
    "\n",
    "Defines the GridWorld mechanics:\n",
    "- Converts between a 1D state index and (row, col)\n",
    "- Applies an action with boundary handling (out-of-grid moves keep the agent in the same state)\n",
    "- Uses reward -1 for every action attempt\n",
    "- Treats the terminal state as absorbing with reward 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e1148ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_rc(s: int, n: int) -> tuple[int, int]:\n",
    "    \"\"\"Convert state index to (row, col).\"\"\"\n",
    "    return divmod(s, n)\n",
    "\n",
    "def rc_to_state(r: int, c: int, n: int) -> int:\n",
    "    \"\"\"Convert (row, col) back to state index.\"\"\"\n",
    "    return r * n + c\n",
    "\n",
    "def step(s: int, action: tuple[int, int], n: int) -> tuple[int, float]:\n",
    "    \"\"\"\n",
    "    Deterministic transition:\n",
    "    - Terminal is absorbing with reward 0.\n",
    "    - Every action attempt yields reward -1.\n",
    "    - If action hits a boundary, agent stays in same state.\n",
    "    \"\"\"\n",
    "    if s == TERMINAL_STATE:\n",
    "        return s, 0.0\n",
    "\n",
    "    r, c = state_to_rc(s, n)\n",
    "    dr, dc = action\n",
    "    nr, nc = r + dr, c + dc\n",
    "\n",
    "    # Boundary handling: stay in place if out-of-bounds\n",
    "    if nr < 0 or nr >= n or nc < 0 or nc >= n:\n",
    "        ns = s\n",
    "    else:\n",
    "        ns = rc_to_state(nr, nc, n)\n",
    "\n",
    "    return ns, -1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2218b41d",
   "metadata": {},
   "source": [
    "## Iterative Bellman updates\n",
    "\n",
    "Runs iterative policy evaluation:\n",
    "1. Initialize \\(V(s)=0\\) for all states.\n",
    "2. For each non-terminal state, update using the Bellman expectation equation under a uniform random policy:  \n",
    "   \\(V_{\\text{new}}(s) = \\frac{1}{4}\\sum_{a}\\left[r(s,a) + \\gamma V(s')\\right]\\)\n",
    "3. Track the maximum absolute change across states.\n",
    "4. Stop when the maximum change is below \\(\\theta = 1e{-4}\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "748d6b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize V(s) = 0 for all states\n",
    "V = np.zeros(N * N, dtype=np.float64)\n",
    "\n",
    "iters = 0\n",
    "while True:\n",
    "    iters += 1\n",
    "    delta = 0.0\n",
    "    V_new = V.copy()\n",
    "\n",
    "    for s in range(N * N):\n",
    "        # Keep terminal state fixed at 0\n",
    "        if s == TERMINAL_STATE:\n",
    "            V_new[s] = 0.0\n",
    "            continue\n",
    "\n",
    "        # Bellman expectation update for uniform random policy\n",
    "        v_s = 0.0\n",
    "        for a in ACTIONS:\n",
    "            ns, r = step(s, a, N)\n",
    "            v_s += ACTION_PROB * (r + GAMMA * V[ns])\n",
    "\n",
    "        V_new[s] = v_s\n",
    "        delta = max(delta, abs(V_new[s] - V[s]))\n",
    "\n",
    "    V = V_new\n",
    "\n",
    "    # Convergence check\n",
    "    if delta < THETA:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c59d54b",
   "metadata": {},
   "source": [
    "## Final output\n",
    "\n",
    "Reshapes the learned values into a 4×4 grid and prints:\n",
    "- Iteration count\n",
    "- Final max change (delta)\n",
    "- The value function matrix (terminal cell should be 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2902547a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 471\n",
      "Last max delta: 9.891255676564015e-05\n",
      "[[-59.42367735 -57.42387125 -54.2813141  -51.71012579]\n",
      " [-57.42387125 -54.56699476 -49.71029394 -45.13926711]\n",
      " [-54.2813141  -49.71029394 -40.85391609 -29.99766609]\n",
      " [-51.71012579 -45.13926711 -29.99766609   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "V_grid = V.reshape(N, N)\n",
    "\n",
    "print(\"Iterations:\", iters)\n",
    "print(\"Last max delta:\", delta)\n",
    "print(V_grid)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
